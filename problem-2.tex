\documentclass[12pt, letterpaper]{article}
\usepackage{graphicx}
\graphicspath{ {Images/} }

\begin{document}
\textbf{\large Assignment 2}\newline
	
\textbf{By Brandon Young and Ruicheng Wu}

\section{problem 2}
\subsection{a.}

We claim the provided tree correctly categorize the provided example since every example can be inducted from this decision tree.
Like GPA above 3.6 is P and below 3.3 is N, then with publication P,otherwise check University Rank, Only rank 2 will be P,other ranks are N. And recommendation doesn't matter.

\subsection{b.}\large
\textbf{Step I}

\textbf{I($\frac{6}{12},\frac{6}{12}$)=1}\newline
GPA: $[3.9,4.0]$ $3(PPP)$ , $(3.2,3.9)$ $5(PPPNN)$ , $[3.0,3.2]$ $4(NNNN)$\newline
University: Rank 1| 5(PPPNN),Rank 2| 3(PPN) , Rank 3| 4(PNNN)\newline
Publication: Yes 5(PPPNN) , No 7(PPPNNNN)\newline
Recommendation: good 8(PPPPPNNN) , normal 4(PNNN)\newline\newline
Gain(GPA)=1-[$\frac{3}{12}$ B($\frac{3}{3}$)+$\frac{5}{12}$ B($\frac{3}{5}$)+$\frac{4}{12}$ B($\frac{0}{4}$)]=1-[0.0+0.404562747689+0.0]
= 0.595437252311\newline\newline
Gain(University)=1-[$\frac{5}{12}$ B($\frac{3}{5}$)+$\frac{3}{12}$ B($\frac{2}{3}$)+$\frac{4}{12}$ B($\frac{1}{4}$)]=\newline1-[0.404562747689+0.229573958514+0.270426041486] =0.095437252395\newline\newline
Gain(Publication)=1-[$\frac{5}{12}$ B($\frac{3}{5}$)+$\frac{7}{12}$ B($\frac{3}{7}$)]=1-[0.404562747689+0.574716412687]= 0.020720839624\newline\newline
Gain(Recommendation)=\newline1-[$\frac{8}{12}$ B($\frac{5}{8}$)+$\frac{4}{12}$ B($\frac{1}{4}$)]=1-[0.636289335283+0.270426041486]= 0.093284623231\newline

So we pick GPA as the best Gain attribute in this level

\includegraphics[scale=0.8]{"problem-2-step1"}
\textbf{Step II}

\textbf{I($\frac{2}{5},\frac{3}{5}$)=0.970950594455}\newline
University: Rank 1| 2(PN),Rank 2| 1(P) , Rank 3| 2(PN)\newline
Publication: Yes 2(PP) , No 3(PNN)\newline
Recommendation: good 5(PPPNN)\newline\newline
Gain(University)=0.970950594455-[$\frac{2}{5}$ B($\frac{1}{2}$)+$\frac{1}{5}$ B($\frac{1}{1}$)+$\frac{2}{5}$ B($\frac{1}{2}$)]=\newline0.970950594455-[0.4+0.0+0.4]
= 0.170950594455\newline\newline
Gain(Publication)=0.970950594455-[$\frac{3}{5}$ B($\frac{1}{3}$)+$\frac{2}{5}$ B($\frac{2}{2}$)]=0.970950594455-[0.550977500433+0.0]=
0.419973094022\newline\newline
Gain(Recommendation)=0.970950594455-[$\frac{5}{5}$ B($\frac{3}{5}$)]=0.970950594455-[0.970950594455+0.0+0.0]
= 0\newline

So we pick Publication as the best Gain attribute in this level

\includegraphics[scale=0.8]{"problem-2-step2"}
\textbf{Step III}

\textbf{I($\frac{1}{3},\frac{2}{3}$)=0.918295834054}\newline	
University: Rank 1 | 1(N),Rank 2 | 1(P) , Rank 3 | 1(N)\newline
Recommendation: good 3(PNN)\newline
Gain(University)=0.918295834054-[$\frac{1}{3}$ B($\frac{0}{1}$)+$\frac{1}{3}$ B($\frac{1}{1}$)+$\frac{1}{3}$ B($\frac{0}{1}$)]=0.918295834054-[0.0+0.0+0.0]=0.918295834054\newline\newline
Gain(Recommendation)=0.918295834054-[$\frac{3}{3}$ B($\frac{1}{3}$)]=0.918295834054-[0.918295834054]= 0\newline

So we pick University as the best Gain attribute in this level

\includegraphics[scale=0.8]{"problem-2-step3"}


And the final tree to be returned is

\includegraphics[scale=0.6]{"problem-2-final"}

\subsection{c.}

The decision tree we got from b. is same from the provided one.
This is not coincidence, it is believed both tree is generated by applying the decision tree algorithm.


\section{problem 3}

\subsection{a.}
\textbf{The horizontal line is $x_{1}$,the vertical line is $x_{2}$.}

\includegraphics[scale=0.6]{"problem-3-a-copy"}

\subsection{b.}
$s_{1}:(1,5)$ , $s_{2}:(-1,3)$ , $w:(1,1)$, 
So decision boundary is: $y = (1,1)x-4$
Assume there is $(a,a)$ for $w^{T}$, we will have
$$a+5a+b=-1$$
$$-a+3a+b=1$$
Solve those equations for : $$a=-0.5,b=2$$	

So we have $w^{T}:(-0.5,-0.5)$ and $b:2$ for parameters.
We can confirm the max Margin is $\frac{2}{||w^{T}||}=\frac{2}{(\frac{\sqrt{2}}{2})}=2\sqrt{2}$, which verifies with the distance between $s_{1}:(1,5)$ , $s_{2}:(-1,3)$ , $\sqrt{(1-(-1))^{2}+(5-3)^{2}}=2\sqrt{2}$.

And also we verify with given data.

[-1 $3]^{T}$ [0 $2]^{T}$ [0 $1]^{T}$ [0 $0]^{T}$, plug in:
All positive , so those are 1;

[1 $5]^{T}$ [1 $6]^{T}$ [3 $3]^{T}$ , plug in:
All negative , so those are -1;



\subsection{c.}
By inspection, the new added points are not support vectors(ie. the closest points to separating line)

We claim the $w^{T}:(-0.5,-0.5)$ and $b:2$ are still same to parametrize with new data. 

[-2 $0]^{T}$ [-2 $1]^{T}$ [-2 $3]^{T}$ [-1 $0]^{T}$ [-1 $1]^{T}$ [0 $0]^{T}$, plug in:
All positive , so those are 1;

\includegraphics[scale=0.6]{"problem-3-c"}

\section{problem 5}
\subsection{a.}

We define those points by inspection.

Class 1: (0.03,0.50),(0.11,1.2),(0.5,0.75),(0.54,0.23),(0.7,0.65),(0.8,0.25),(0.91,0.44);

Class -1: (0.07,0.73),(0.23,0.49),(0.24,0.33),(0.35,0.35),(0.46,0.46);

The line can be observed from the graph, and we found the best single perceptron will have two unclassied points. So the minimum error will be
$\frac{2}{12}= 0.167; $

The formula for this dividing line is : $1-1*x_{1}-1*x_{2}=0$

\includegraphics[scale=0.6]{"problem-5a"}

\subsection{b.}

We claim that we need 3 separate lines to completely separate two classes. classes. So we need at least 3 perceptrons
to compute classification functions. We will just use 3 perceptrons for simplicity.\newline
\newline
Formula for line 1:\newline
$1-1*x_{1}-1*x_{2}=0$\newline
$W_{1,0}=1,W_{1,1}=-1,W_{1,2}=-1$\newline
Formula for line 2:\newline
$0.2-1*x_{1}+0.6*x_{2}=0$\newline
$W_{1,0}=0.2,W_{1,1}=-1,W_{1,2}=0.6$\newline
Formula for line 3:\newline
$-0.2+1*x_{1}+0.2*x_{2}=0$\newline
$W_{1,0}=-0.2,W_{1,1}=1,W_{1,2}=0.2$\newline

\includegraphics[scale=0.6]{"problem-5b-1"}

As we can see, a data point gives output as positive with all three perceptrons is class 1 and same applies to class -1.
So in the next layer, we can simply apply "and" operation to the results of all thre eperceptrons. In that case, we only need one unit. And the output will be 1 if and only if all perceptrons in 1st layer output 1.

Hence we set $$W_{4,0}=-2.5,W_{4,1}=1,W_{4,2}=1,W_{4,3}=1$$\newline

\includegraphics[scale=0.6]{"problem-5b-2"}




\end{document}