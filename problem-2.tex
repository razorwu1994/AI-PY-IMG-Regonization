\documentclass[12pt, letterpaper]{article}
\usepackage{graphicx}
\graphicspath{ {Images/} }

\begin{document}
\textbf{\large Assignment 2}\newline
	
\textbf{By Brandon Young and Ruicheng Wu}

\section{problem 2}
\subsection{a.}

We claim the provided tree correctly categorize the provided example since every example can be inducted from this decision tree.
Like GPA above 3.6 is P and below 3.3 is N, then with publication P,otherwise check University Rank, Only rank 2 will be P,other ranks are N. And recommendation doesn't matter.

\subsection{b.}\large
\textbf{Step I}

\textbf{I($\frac{6}{12},\frac{6}{12}$)=1}\newline
GPA: $[3.9,4.0]$ $3(PPP)$ , $(3.2,3.9)$ $5(PPPNN)$ , $[3.0,3.2]$ $4(NNNN)$\newline
University: Rank 1| 5(PPPNN),Rank 2| 3(PPN) , Rank 3| 4(PNNN)\newline
Publication: Yes 5(PPPNN) , No 7(PPPNNNN)\newline
Recommendation: good 8(PPPPPNNN) , normal 4(PNNN)\newline\newline
Gain(GPA)=1-[$\frac{3}{12}$ B($\frac{3}{3}$)+$\frac{5}{12}$ B($\frac{3}{5}$)+$\frac{4}{12}$ B($\frac{0}{4}$)]=1-[0.0+0.404562747689+0.0]
= 0.595437252311\newline\newline
Gain(University)=1-[$\frac{5}{12}$ B($\frac{3}{5}$)+$\frac{3}{12}$ B($\frac{2}{3}$)+$\frac{4}{12}$ B($\frac{1}{4}$)]=\newline1-[0.404562747689+0.229573958514+0.270426041486] =0.095437252395\newline\newline
Gain(Publication)=1-[$\frac{5}{12}$ B($\frac{3}{5}$)+$\frac{7}{12}$ B($\frac{3}{7}$)]=1-[0.404562747689+0.574716412687]= 0.020720839624\newline\newline
Gain(Recommendation)=\newline1-[$\frac{8}{12}$ B($\frac{5}{8}$)+$\frac{4}{12}$ B($\frac{1}{4}$)]=1-[0.636289335283+0.270426041486]= 0.093284623231\newline

So we pick GPA as the best Gain attribute in this level

\includegraphics[scale=0.8]{"problem-2-step1"}
\textbf{Step II}

\textbf{I($\frac{2}{5},\frac{3}{5}$)=0.970950594455}\newline
University: Rank 1| 2(PN),Rank 2| 1(P) , Rank 3| 2(PN)\newline
Publication: Yes 2(PP) , No 3(PNN)\newline
Recommendation: good 5(PPPNN)\newline\newline
Gain(University)=0.970950594455-[$\frac{2}{5}$ B($\frac{1}{2}$)+$\frac{1}{5}$ B($\frac{1}{1}$)+$\frac{2}{5}$ B($\frac{1}{2}$)]=\newline0.970950594455-[0.4+0.0+0.4]
= 0.170950594455\newline\newline
Gain(Publication)=0.970950594455-[$\frac{3}{5}$ B($\frac{1}{3}$)+$\frac{2}{5}$ B($\frac{2}{2}$)]=0.970950594455-[0.550977500433+0.0]=
0.419973094022\newline\newline
Gain(Recommendation)=0.970950594455-[$\frac{5}{5}$ B($\frac{3}{5}$)]=0.970950594455-[0.970950594455+0.0+0.0]
= 0\newline

So we pick Publication as the best Gain attribute in this level

\includegraphics[scale=0.8]{"problem-2-step2"}
\textbf{Step III}

\textbf{I($\frac{1}{3},\frac{2}{3}$)=0.918295834054}\newline	
University: Rank 1 | 1(N),Rank 2 | 1(P) , Rank 3 | 1(N)\newline
Recommendation: good 3(PNN)\newline
Gain(University)=0.918295834054-[$\frac{1}{3}$ B($\frac{0}{1}$)+$\frac{1}{3}$ B($\frac{1}{1}$)+$\frac{1}{3}$ B($\frac{0}{1}$)]=0.918295834054-[0.0+0.0+0.0]=0.918295834054\newline\newline
Gain(Recommendation)=0.918295834054-[$\frac{3}{3}$ B($\frac{1}{3}$)]=0.918295834054-[0.918295834054]= 0\newline

So we pick University as the best Gain attribute in this level

\includegraphics[scale=0.8]{"problem-2-step3"}


And the final tree to be returned is

\includegraphics[scale=0.6]{"problem-2-final"}

\subsection{c.}

The decision tree we got from b. is same from the provided one.
This is not coincidence, it is believed both tree is generated by applying the decision tree algorithm.


\section{problem 3}

\subsection{a.}
\includegraphics[scale=0.8]{"problem-3-a"}

\subsection{b.}
w:(-1,1)
b:(4,0)

\subsection{c.}

\end{document}