\documentclass[12pt, letterpaper]{article}
\usepackage{graphicx}
\graphicspath{ {Images/} }

\begin{document}
	\textbf{\large Assignment 2 Supervised Learning}\newline
	
	\textbf{By Brandon Young and Ruicheng Wu}
	\section{problem 1}
	
	\paragraph{1.3\newline}
	\includegraphics[scale=0.8]{"problem-1-3"}
	
	\paragraph{1.4\newline}
	\includegraphics[scale=0.8]{"problem-1-4"}
	
	\bigskip
	From the graph above, with smaller training sets, the perceptron performs the worse (43\% error) and the multi-layer neural network (MLP) does better with a 36\% error rate and lastly SVM performs the best (24\% error). Soon after the amount of training examples increases, the perceptron outperforms the neural network, perhaps because the perceptron can learn the weights quicker.
	
	With enough training examples, the perceptron performs the worse, with a 25\% error rate and SVM comes in at 2nd with an error rate of about 18\%. The neural network performs the best with a 16\% error rate.
	
	In short, the neural network performs the best, although all three methods improve with larger training sets and perform reasonably well with enough data.
	
	\pagebreak
	%-----------------------------------------------------------------------
	\section{problem 2}
	\subsection{a.}
	
	We claim that the provided tree correctly categorizes the provided examples since every example's class can be determined from this decision tree. Examples 1, 2 and 3 with a GPA above 3.8 are in P and those with a GPA below 3.3 (examples 9, 10, 11 and 12) are in N. 
	
	For the rest of the examples, examples 4 and 6 had prior research and they are in P, otherwise check their University Rank. That leaves examples 5, 7 and 8. Examples 8 and 7 have rank 1 and 3 respectively and are correctly placed in the N class. Lastly example 5 has rank 2 and is accepted. Recommendation doesn't matter according to this tree
	
	\subsection{b.}\large
	\textbf{Step I}
	
	\textbf{I($\frac{6}{12},\frac{6}{12}$)=1}\newline
	GPA: $[3.9,4.0]$ $3(PPP)$ , $(3.2,3.9)$ $5(PPPNN)$ , $[3.0,3.2]$ $4(NNNN)$\newline
	University: Rank 1| 5(PPPNN),Rank 2| 3(PPN) , Rank 3| 4(PNNN)\newline
	Publication: Yes 5(PPPNN) , No 7(PPPNNNN)\newline
	Recommendation: good 8(PPPPPNNN) , normal 4(PNNN)\newline\newline
	Gain(GPA)=1-[$\frac{3}{12}$ B($\frac{3}{3}$)+$\frac{5}{12}$ B($\frac{3}{5}$)+$\frac{4}{12}$ B($\frac{0}{4}$)]=1-[0.0+0.404562747689+0.0]
	= 0.595437252311\newline\newline
	Gain(University)=1-[$\frac{5}{12}$ B($\frac{3}{5}$)+$\frac{3}{12}$ B($\frac{2}{3}$)+$\frac{4}{12}$ B($\frac{1}{4}$)]=\newline1-[0.404562747689+0.229573958514+0.270426041486] =0.095437252395\newline\newline
	Gain(Publication)=1-[$\frac{5}{12}$ B($\frac{3}{5}$)+$\frac{7}{12}$ B($\frac{3}{7}$)]=1-[0.404562747689+0.574716412687]= 0.020720839624\newline\newline
	Gain(Recommendation)=\newline1-[$\frac{8}{12}$ B($\frac{5}{8}$)+$\frac{4}{12}$ B($\frac{1}{4}$)]=1-[0.636289335283+0.270426041486]= 0.093284623231\newline
	
	So we pick GPA as the best Gain attribute in this level
	
	\includegraphics[scale=0.8]{"problem-2-step1"}
	\textbf{Step II}
	
	\textbf{I($\frac{2}{5},\frac{3}{5}$)=0.970950594455}\newline
	University: Rank 1| 2(PN),Rank 2| 1(P) , Rank 3| 2(PN)\newline
	Publication: Yes 2(PP) , No 3(PNN)\newline
	Recommendation: good 5(PPPNN)\newline\newline
	Gain(University)=0.970950594455-[$\frac{2}{5}$ B($\frac{1}{2}$)+$\frac{1}{5}$ B($\frac{1}{1}$)+$\frac{2}{5}$ B($\frac{1}{2}$)]=\newline0.970950594455-[0.4+0.0+0.4]
	= 0.170950594455\newline\newline
	Gain(Publication)=0.970950594455-[$\frac{3}{5}$ B($\frac{1}{3}$)+$\frac{2}{5}$ B($\frac{2}{2}$)]=0.970950594455-[0.550977500433+0.0]=
	0.419973094022\newline\newline
	Gain(Recommendation)=0.970950594455-[$\frac{5}{5}$ B($\frac{3}{5}$)]=0.970950594455-[0.970950594455+0.0+0.0]
	= 0\newline
	
	So we pick Publication as the best Gain attribute in this level
	
	\includegraphics[scale=0.8]{"problem-2-step2"}
	\textbf{Step III}
	
	\textbf{I($\frac{1}{3},\frac{2}{3}$)=0.918295834054}\newline	
	University: Rank 1 | 1(N),Rank 2 | 1(P) , Rank 3 | 1(N)\newline
	Recommendation: good 3(PNN)\newline
	Gain(University)=0.918295834054-[$\frac{1}{3}$ B($\frac{0}{1}$)+$\frac{1}{3}$ B($\frac{1}{1}$)+$\frac{1}{3}$ B($\frac{0}{1}$)]=0.918295834054-[0.0+0.0+0.0]=0.918295834054\newline\newline
	Gain(Recommendation)=0.918295834054-[$\frac{3}{3}$ B($\frac{1}{3}$)]=0.918295834054-[0.918295834054]= 0\newline
	
	So we pick University as the best Gain attribute in this level
	
	\includegraphics[scale=0.8]{"problem-2-step3"}
	
	
	And the final tree to be returned is
	
	\includegraphics[scale=0.6]{"problem-2-final"}
	
	\subsection{c.}
	
	The decision tree we got from b. is same from the provided one.
	This is not coincidence, it is believed both tree is generated by applying the decision tree algorithm.
	
	\pagebreak
	%-----------------------------------------------------------------------
	\section{problem 3}
	
	\subsection{a.}
	\textbf{The horizontal line is $x_{1}$,the vertical line is $x_{2}$.}
	
	\includegraphics[scale=0.6]{"problem-3-a-copy"}
	
	\subsection{b.}
	$s_{1}:(1,5)$ , $s_{2}:(-1,3)$ , $w:(1,1)$, 
	So decision boundary is: $y = (1,1)x-4$
	Assume there is $(a,a)$ for $w^{T}$, we will have
	$$a+5a+b=-1$$
	$$-a+3a+b=1$$
	Solve those equations for : $$a=-0.5,b=2$$	
	
	So we have $w^{T}:(-0.5,-0.5)$ and $b:2$ for parameters.
	We can confirm the max Margin is $\frac{2}{||w^{T}||}=\frac{2}{(\frac{\sqrt{2}}{2})}=2\sqrt{2}$, which verifies with the distance between $s_{1}:(1,5)$ , $s_{2}:(-1,3)$ , $\sqrt{(1-(-1))^{2}+(5-3)^{2}}=2\sqrt{2}$.
	
	And also we verify with given data.
	
	[-1 $3]^{T}$ [0 $2]^{T}$ [0 $1]^{T}$ [0 $0]^{T}$, plug in:
	All positive , so those are 1;
	
	[1 $5]^{T}$ [1 $6]^{T}$ [3 $3]^{T}$ , plug in:
	All negative , so those are -1;
	
	
	
	\subsection{c.}
	By inspection, the new added points are not support vectors(ie. the closest points to separating line)
	
	We claim the $w^{T}:(-0.5,-0.5)$ and $b:2$ are still same to parametrize with new data. 
	
	[-2 $0]^{T}$ [-2 $1]^{T}$ [-2 $3]^{T}$ [-1 $0]^{T}$ [-1 $1]^{T}$ [0 $0]^{T}$, plug in:
	All positive , so those are 1;
	
	\includegraphics[scale=0.6]{"problem-3-c"}
	
	
	%-----------------------------------------------------------------------
	\pagebreak
	\section{problem 4}
	\subsection{a.}
	The initial linear separator (line 1) is $i_2 = -\frac{i_1}{i_2} - \frac{0.2}{i_2}$ or $i_2 = i_1 + 0.2$:
	\medskip
	
	\includegraphics[scale=0.6]{"Problem 4/Initial Linear Separator"}
	
	\medskip 
	\noindent Assume that the points are:
	\begin{itemize}
		\item A: $(0.08, 0.72)$
		\item B: $(0.28, 0.58)$
		\item C: $(0.44, 0.15)$
		\item D: $(0.6, 0.3)$
		\item E: $(0.12, 1.05)$
		\item F: $(0.35, 0.98)$
		\item G: $(0.7, 0.65)$
		\item H: $(0.92, 0.44)$	
	\end{itemize}
	where A, B, C and D are in class 1 and E, F, G and H are in class -1. 	 
	
	Use $f(x) = w_1i_1+w_2i_2 + 0.2$ to classify the samples. If $f(x)<0$ then x is in class -1 and $h_w(x) = 0$. If $f(x) \geq 0$ then x is in class 1 and $h_w(x) = 1$. By the initial linear separator, 4 samples are misclassified (A, B, G and H):
	\begin{itemize}
		\item A: $(0.08) - (0.72) + 0.2 = -0.44 < 0, h_w(A) = 0$
		\item B: $(0.28) - (0.58) + 0.2 = -0.1 < 0, h_w(B) = 0$
		\item C: $(0.44) - (0.15) + 0.2 = 0.49 > 0, h_w(C) = 1$
		\item D: $(0.6) - (0.3) + 0.2 = 0.5 > 0, h_w(D) = 1$
		\item E: $(0.12) - (1.05) + 0.2 = -0.73 < 0, h_w(E) = 0$
		\item F: $(0.35) - (0.98) + 0.2 = -0.43 < 0, h_w(F) = 0$
		\item G: $(0.7) - (0.65) + 0.2 = 0.25 > 0, h_w(G) = 1$
		\item H: $(0.92) - (0.44) + 0.2 = 0.68 > 0, h_w(H) = 1$	
	\end{itemize} 	
	Let the learning rate, $\alpha = 0.5$ and use A to update the weights. Then $Err_A = y - h_w(A) = 1 - 0 = 1$.
	$$w_1' \leftarrow w_1 + \alpha * Err_A * i_1$$
	$$w_1' \leftarrow 1 + (0.5)(1)(0.08)$$
	$$w_1' = 1.04$$
	
	$$w_2' \leftarrow (-1) + (0.5)(1)(0.72)$$
	$$w_2' = -0.64$$
	Hence the new weights are $w_1 = 1.04$ and $w_2 = -0.64$. Line 2, then, is: $i_2 = 1.625(i_1) + 0.3125$, with 3 misclassified points:
	\medskip
	
	\includegraphics[scale=0.6]{"Problem 4/Line 2"}
	
	\medskip 
	Following a similar process as above, the next few iterations generate the following plots:
	
	Update Line 2 with A to get $w_1 = 1.08$, $w_2 = -0.28$. Line 3: $i_2 = 3.857i_1 + 0.714$, with 4 misclassified points:
	\medskip

	\includegraphics[scale=0.6]{"Problem 4/Line 3"}
	
	\medskip 	
	Update Line 3 with E to get $w_1 = 1.02$, $w_2 = -0.805$. Line 4, $i_2 = 1.267(i_1) + 0.248$, with 3 misclassified points:
	\medskip

	\includegraphics[scale=0.6]{"Problem 4/Line 4"}
	
	\medskip 	
	Update Line 4 with A to get $w_1 = 1.06$, $w_2 = -0.445$. Line 5, $i_2 = 2.382(i_1) + 0.449$, with 4 misclassified points:
	\medskip
	
	\includegraphics[scale=0.6]{"Problem 4/Line 5"}
	
	\medskip 	
	Update Line 5 with A to get $w_1 = 1.1$, $w_2 = -0.085$. Line 6, $i_2 = 12.941(i_1) + 2.35$, with 4 misclassified points:
	\medskip

	\includegraphics[scale=0.6]{"Problem 4/Line 6"}	
	
	\subsection{b.}
	The linear separator that achieves perfect classification (found after 172 iterations) is:
	$$i_2 = -0.58333i_1 + 0.83333$$
	where $w_1 = -0.14$ and $w_2 = -0.24$. The line in the graph:
	\medskip

	\includegraphics[scale=0.6]{"Problem 4/Best Line"}
	
	\medskip 
	
	
	\subsection{c.}
	\includegraphics[scale=1]{"Problem 4/1D Diagram"}	
	
	The diagram above shows only the $i_1$ value from each sample. The dotted line represents the location of the best possible split, which is at $i_1 = 0.65$, with only 2 misclassified points and a $25\%$ error. 
	
	A separator in 1D is $w_1i_1 + w_0i_0$ or $w_1i_1 + 0.2$ (assuming $w_0 = 0.2$) which can be used to solve for $w_1$:
	$$w_1 = -\frac{0.2}{i_1} = -\frac{0.2}{0.65} = -0.3077$$ 
	As a result, $w_1 = -0.3077$ is the weight that best classifies the samples.
	
	%-----------------------------------------------------------------------
	\pagebreak
	\section{problem 5}
	\subsection{a.}
	
	We define those points by inspection.
	
	Class 1: (0.03,0.50),(0.11,1.2),(0.5,0.75),(0.54,0.23),(0.7,0.65),(0.8,0.25),(0.91,0.44);
	
	Class -1: (0.07,0.73),(0.23,0.49),(0.24,0.33),(0.35,0.35),(0.46,0.46);
	
	The line can be observed from the graph, and we found the best single perceptron will have two unclassied points. So the minimum error will be
	$\frac{2}{12}= 0.167; $
	
	The formula for this dividing line is : $1-1*x_{1}-1*x_{2}=0$
	
	\includegraphics[scale=0.6]{"problem-5a"}
	
	\subsection{b.}
	
	We claim that we need 3 separate lines to completely separate two classes. classes. So we need at least 3 perceptrons
	to compute classification functions. We will just use 3 perceptrons for simplicity.\newline
	\newline
	Formula for line 1:\newline
	$1-1*x_{1}-1*x_{2}=0$\newline
	$W_{1,0}=1,W_{1,1}=-1,W_{1,2}=-1$\newline
	Formula for line 2:\newline
	$0.2-1*x_{1}+0.6*x_{2}=0$\newline
	$W_{1,0}=0.2,W_{1,1}=-1,W_{1,2}=0.6$\newline
	Formula for line 3:\newline
	$-0.2+1*x_{1}+0.2*x_{2}=0$\newline
	$W_{1,0}=-0.2,W_{1,1}=1,W_{1,2}=0.2$\newline
	
	\includegraphics[scale=0.6]{"problem-5b-1"}
	
	As we can see, a data point gives output as positive with all three perceptrons is class 1 and same applies to class -1.
	So in the next layer, we can simply apply "and" operation to the results of all thre eperceptrons. In that case, we only need one unit. And the output will be 1 if and only if all perceptrons in 1st layer output 1.
	
	Hence we set $$W_{4,0}=-2.5,W_{4,1}=1,W_{4,2}=1,W_{4,3}=1$$\newline
	
	\includegraphics[scale=0.6]{"problem-5b-2"}
	
	
	
	
\end{document}